{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_faces(video_path, output_folder, label):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # frame_rate = cap.get()  # Frame rate\n",
    "    count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        # frame_id = cap.get(1)  # Current frame number\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # if frame_id % int(frame_rate) == 0:\n",
    "            # Convert frame to grayscale for face detection\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract and resize the face region\n",
    "            face = frame[y:y + h, x:x + w]\n",
    "            resized_face = cv2.resize(face, (224, 224))\n",
    "\n",
    "            # Save the resized face\n",
    "            output_path = os.path.join(output_folder, f\"{label}_face_{count}.jpg\")\n",
    "            cv2.imwrite(output_path, resized_face)\n",
    "\n",
    "            count += 1\n",
    "            # Skip frames based on frame rate\n",
    "            # frames_to_skip = int(frame_rate) - 1\n",
    "            # for _ in range(frames_to_skip):\n",
    "            #     cap.read()\n",
    "\n",
    "    cap.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "real_video_path = \"original\"\n",
    "fake_video_path = \"fake\"\n",
    "og_folder = \"og_images\"\n",
    "fake_folder = \"fake_images\"\n",
    "\n",
    "# Extract faces from real videos\n",
    "for video_file in os.listdir(real_video_path):\n",
    "    video_path = os.path.join(real_video_path, video_file)\n",
    "    extract_faces(video_path, og_folder, label=\"real\")\n",
    "\n",
    "# Extract faces from fake videos\n",
    "for video_file in os.listdir(fake_video_path):\n",
    "    video_path = os.path.join(fake_video_path, video_file)\n",
    "    extract_faces(video_path, fake_folder, label=\"fake\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vinni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = load_img(img_path, target_size=IMAGE_SIZE)\n",
    "        img_array = img_to_array(img)\n",
    "        images.append(img_array)\n",
    "        label = filename.split('_')[0]\n",
    "        labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess images\n",
    "# real_images, real_labels = load_images_from_folder(\"path/to/preprocessed_faces/real\")\n",
    "# fake_images, fake_labels = load_images_from_folder(\"path/to/preprocessed_faces/fake\")\n",
    "\n",
    "images, labels=load_images_from_folder(\"outputimages\")\n",
    "# Combine real and fake data\n",
    "# images = np.concatenate((real_images, fake_images), axis=0)\n",
    "# labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "encoded_labels = to_categorical(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.1206 - accuracy: 0.0000e+00 - val_loss: 2482.8218 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4710.0161 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6887.2910 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9021.4766 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 11109.9531 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 13127.8984 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 15087.3408 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 17015.2812 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 18893.7266 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 20701.0059 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 113ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Classification report\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2154\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2151\u001b[0m             )\n\u001b[0;32m   2152\u001b[0m         )\n\u001b[0;32m   2153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2158\u001b[0m         )\n\u001b[0;32m   2159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2160\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 742 images belonging to 2 classes.\n",
      "Found 185 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 4s 119ms/step - loss: 0.3652 - accuracy: 0.8521 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 2s 86ms/step - loss: 0.0363 - accuracy: 0.9845 - val_loss: 3.0825e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 2s 87ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0371e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 2s 88ms/step - loss: 2.2781e-04 - accuracy: 1.0000 - val_loss: 1.9535e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 2s 90ms/step - loss: 2.3154e-04 - accuracy: 1.0000 - val_loss: 1.8219e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 2s 87ms/step - loss: 5.8794e-05 - accuracy: 1.0000 - val_loss: 7.9766e-06 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 2s 85ms/step - loss: 4.3431e-05 - accuracy: 1.0000 - val_loss: 6.4230e-06 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 2s 84ms/step - loss: 4.0410e-05 - accuracy: 1.0000 - val_loss: 4.6771e-06 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 2s 85ms/step - loss: 3.6287e-05 - accuracy: 1.0000 - val_loss: 5.4592e-06 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 2s 85ms/step - loss: 3.2233e-05 - accuracy: 1.0000 - val_loss: 5.0322e-06 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Image dimensions\n",
    "img_width, img_height = 64, 64  # Adjust based on your dataset\n",
    "\n",
    "train_data_dir = 'images/'  # Path to the parent folder containing 'original' and 'fake' folders\n",
    "batch_size = 32\n",
    "\n",
    "# Data generator for training\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # 20% of data for validation\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation')\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # use 'softmax' for more than two classes\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size)\n",
    "\n",
    "# Save the model\n",
    "model.save('deepfake_detection_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 28ms/step\n",
      "The video is predicted as: REAL\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def extract_frames(video_path, size=(64, 64)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, size)\n",
    "        frame = img_to_array(frame)\n",
    "        frame = np.expand_dims(frame, axis=0)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.vstack(frames)\n",
    "\n",
    "def predict_video(video_path, model):\n",
    "    frames = extract_frames(video_path)\n",
    "    # Assuming that your model was trained with normalized images\n",
    "    frames = frames.astype('float32') / 255.0\n",
    "    predictions = model.predict(frames)\n",
    "    avg_prediction = np.mean(predictions)\n",
    "    return 'FAKE' if avg_prediction > 0.5 else 'REAL'\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model('deepfake_detection_model.h5')\n",
    "\n",
    "# Path to the new video\n",
    "new_video_path = 'id9_0009.mp4'\n",
    "\n",
    "# Predict\n",
    "video_label = predict_video(new_video_path, model)\n",
    "print(f\"The video is predicted as: {video_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
