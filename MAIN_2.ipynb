{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Problem** : I have an imbalanced dataset with 500 original videos and 5,000 fake videos that I want to use for deepfake detection. Extracting frames from each video would result in far more fake frames, skewing the training distribution.\n",
    "- **Solution** : To mitigate this class imbalance while retaining diversity, I can restrict the number of frames per fake video. For example, sampling only 10 frames evenly across each fake video results in 5,000 fake frames to match the 500 original videos. This maintains equal class representation during training.\n",
    "- Another possible solution is data augmentation but considering the storage in mind this is the best possible option we have if this doesn't work then we will use data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This the V2 of the code for deep fake detector\n",
    "- main change introduced we introduced the capability to split the videos into frames and selecting the frames based on variance rather than selecting them randomly\n",
    "- The key is to ensure that these frames are representative of the entire video. You can select frames evenly spaced throughout the video, or use a more sophisticated method like selecting frames that have the most variance or are most representative of the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## video ---> frame splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def extract_faces_with_variance(video_path, output_folder, label, max_frames=None):\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    try:\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Unable to open video: {video_path}\")\n",
    "            return\n",
    "\n",
    "        frame_variance = []\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            var = np.var(gray_frame)\n",
    "            frame_variance.append((var, frame))\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Select frames based on the max_frames parameter\n",
    "        if max_frames and max_frames > 0:\n",
    "            frame_variance.sort(key=lambda x: x[0], reverse=True)\n",
    "            selected_frames = frame_variance[:max_frames]\n",
    "        else:\n",
    "            selected_frames = frame_variance\n",
    "\n",
    "        faces_detected = False\n",
    "        count = 0\n",
    "        for _, frame in selected_frames:\n",
    "            faces = face_cascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5)\n",
    "            for (x, y, w, h) in faces:\n",
    "                faces_detected = True\n",
    "                face = frame[y:y + h, x:x + w]\n",
    "                resized_face = cv2.resize(face, (224, 224))\n",
    "                output_path = os.path.join(output_folder, f\"{label}_{video_name}_face_{count}.jpg\")\n",
    "                cv2.imwrite(output_path, resized_face)\n",
    "                count += 1\n",
    "\n",
    "        if not faces_detected:\n",
    "            print(f\"No faces detected in {video_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {video_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "real_video_path = \"original\"\n",
    "fake_video_path = \"fake\"\n",
    "og_folder = \"images/real\"\n",
    "fake_folder = \"images/fake\"\n",
    "\n",
    "# Extract faces from real videos\n",
    "# for video_file in os.listdir(real_video_path):\n",
    "#     video_path = os.path.join(real_video_path, video_file)\n",
    "#     # print(f\"Processing video: {video_path}\")\n",
    "#     extract_faces_with_variance(video_path, og_folder, label=\"real\")\n",
    "\n",
    "# Extract faces from fake videos\n",
    "for video_file in os.listdir(fake_video_path):\n",
    "    video_path = os.path.join(fake_video_path, video_file)\n",
    "    extract_faces_with_variance(video_path, fake_folder, label=\"fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Image dimensions\n",
    "img_width, img_height = 64, 64  # Adjust based on your dataset\n",
    "\n",
    "train_data_dir = 'images/'  # Path to the parent folder containing 'original' and 'fake' folders\n",
    "batch_size = 32\n",
    "\n",
    "# Data generator for training\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # 20% of data for validation\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation')\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # use 'softmax' for more than two classes\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size)\n",
    "\n",
    "# Save the model\n",
    "model.save('deepfake_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 16ms/step\n",
      "The video is predicted as: REAL\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def extract_frames(video_path, size=(64, 64)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, size)\n",
    "        frame = img_to_array(frame)\n",
    "        frame = np.expand_dims(frame, axis=0)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.vstack(frames)\n",
    "\n",
    "def predict_video(video_path, model):\n",
    "    frames = extract_frames(video_path)\n",
    "    # Assuming that your model was trained with normalized images\n",
    "    frames = frames.astype('float32') / 255.0\n",
    "    predictions = model.predict(frames)\n",
    "    avg_prediction = np.mean(predictions)\n",
    "    return 'FAKE' if avg_prediction > 0.5 else 'REAL'\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model('deepfake_detection_model.h5')\n",
    "\n",
    "# Path to the new video\n",
    "new_video_path = 'id9_0009.mp4'\n",
    "\n",
    "# Predict\n",
    "video_label = predict_video(new_video_path, model)\n",
    "print(f\"The video is predicted as: {video_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
